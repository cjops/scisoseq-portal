{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "_con = sqlite3.connect('/mnt/z/Gandal/fetal.db')\n",
    "#_con.row_factory = sqlite3.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('COMPILER=gcc-7.3.0',), ('ENABLE_COLUMN_METADATA',), ('ENABLE_DBSTAT_VTAB',), ('ENABLE_FTS3',), ('ENABLE_FTS3_TOKENIZER',), ('ENABLE_FTS4',), ('ENABLE_FTS5',), ('ENABLE_GEOPOLY',), ('ENABLE_JSON1',), ('ENABLE_RTREE',), ('ENABLE_UNLOCK_NOTIFY',), ('MAX_DEFAULT_PAGE_SIZE=32768',), ('MAX_EXPR_DEPTH=10000',), ('MAX_VARIABLE_NUMBER=250000',), ('SECURE_DELETE',), ('THREADSAFE=1',)]\n"
     ]
    }
   ],
   "source": [
    "print(list(c.execute(\"pragma compile_options;\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f2d6e4d3a40>"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "# Create table\n",
    "_con.executescript(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS datasets (\n",
    "    name TEXT NOT NULL PRIMARY KEY,\n",
    "    date TEXT\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS genes (\n",
    "    chromosome TEXT,\n",
    "    source TEXT,\n",
    "    feature_type TEXT,\n",
    "    start INT,\n",
    "    end INT,\n",
    "    score TEXT,\n",
    "    strand TEXT,\n",
    "    frame TEXT,\n",
    "    attributes TEXT,\n",
    "    dataset TEXT REFERENCES datasets,\n",
    "    gene_id TEXT NOT NULL,\n",
    "    gene_name TEXT,\n",
    "    PRIMARY KEY(gene_id, dataset)\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS transcripts (\n",
    "    chromosome TEXT,\n",
    "    source TEXT,\n",
    "    feature_type TEXT,\n",
    "    start INT,\n",
    "    end INT,\n",
    "    score TEXT,\n",
    "    strand TEXT,\n",
    "    frame TEXT,\n",
    "    attributes TEXT,\n",
    "    dataset TEXT REFERENCES datasets,\n",
    "    gene_id TEXT,\n",
    "    gene_name TEXT,\n",
    "    transcript_id TEXT NOT NULL,\n",
    "    FOREIGN KEY(gene_id, dataset) REFERENCES genes(gene_id, dataset),\n",
    "    PRIMARY KEY(transcript_id, gene_id, dataset)\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS exons (\n",
    "    chromosome TEXT,\n",
    "    source TEXT,\n",
    "    feature_type TEXT,\n",
    "    start INT,\n",
    "    end INT,\n",
    "    score TEXT,\n",
    "    strand TEXT,\n",
    "    frame TEXT,\n",
    "    attributes TEXT,\n",
    "    dataset TEXT REFERENCES datasets,\n",
    "    gene_id TEXT,\n",
    "    gene_name TEXT,\n",
    "    transcript_id TEXT,\n",
    "    exon_id TEXT,\n",
    "    exon_number INT NOT NULL,\n",
    "    FOREIGN KEY(gene_id, dataset) REFERENCES genes(gene_id, dataset),\n",
    "    FOREIGN KEY(transcript_id, gene_id, dataset) REFERENCES transcripts(transcript_id, gene_id, dataset)\n",
    "    PRIMARY KEY(transcript_id, exon_number, gene_id, dataset)\n",
    ");\n",
    "CREATE TABLE IF NOT EXISTS model_exons (\n",
    "    chromosome TEXT,\n",
    "    start INT,\n",
    "    end INT,\n",
    "    strand TEXT,\n",
    "    gene_name TEXT NOT NULL,\n",
    "    exon_id TEXT,\n",
    "    exon_number INT NOT NULL,\n",
    "    PRIMARY KEY(gene_name, exon_number)\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7f2d6e5c06c0>"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "_con.executescript(\"\"\"\n",
    "CREATE INDEX index_genes_gene_name ON genes(gene_name);\n",
    "CREATE INDEX index_transcripts_gene_name ON transcripts(gene_name);\n",
    "CREATE INDEX index_exons_gene_name ON exons(gene_name);\n",
    "CREATE INDEX index_model_exons_gene_name ON model_exons(gene_name);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('2020-12-02',)]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "list(c.execute(\"SELECT date('now');\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'name': 'myygtff', 'date': date.today()}\n",
    "with conn:\n",
    "    conn.execute('INSERT INTO datasets VALUES (?, ?)', tuple(d.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('myygtf', datetime.date(2020, 12, 3), 'hi')"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "d = {'name': 'myygtf', 'date': date.today()}\n",
    "tuple(d.values())+('hi',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exists\n"
     ]
    }
   ],
   "source": [
    "name = 'mygtf'\n",
    "if _con.execute('SELECT * FROM datasets WHERE name=?', (name,)).fetchone():\n",
    "    print('exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gtf_columns = ['chromosome', 'source', 'feature_type', 'start', 'end', 'score', 'strand', 'frame']\n",
    "\n",
    "def parse_gtf_attr(s):\n",
    "    d = {}\n",
    "    for item in s.split(';')[:-1]:\n",
    "        item = item.strip().split(' ', maxsplit=1)\n",
    "        if item[0] in d:\n",
    "            d[item[0]].append(item[1].strip('\"'))\n",
    "        else:\n",
    "            d[item[0]] = [item[1].strip('\"')]\n",
    "    return {k: v if len(v) > 1 else v[0] for k,v in d.items()}\n",
    "\n",
    "def parse_gtf_line(line):\n",
    "    line = line.rstrip()\n",
    "    cols = line.split('\\t')\n",
    "    feature = {\n",
    "        'chromosome': cols[0],\n",
    "        'source': cols[1],\n",
    "        'feature_type': cols[2],\n",
    "        'start': int(cols[3]),\n",
    "        'end': int(cols[4]),\n",
    "        'score': cols[5],\n",
    "        'strand': cols[6],\n",
    "        'frame': cols[7],\n",
    "        'attributes': parse_gtf_attr(cols[8])\n",
    "    }\n",
    "    return feature\n",
    "\n",
    "def remove_ensembl_suffix(id):\n",
    "    if id.startswith('ENS'):\n",
    "        return id.rsplit('.', maxsplit=1)[0]\n",
    "    else:\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_gtf(f, name):\n",
    "    if _con.execute('SELECT * FROM datasets WHERE name=?', (name,)).fetchone():\n",
    "        print('\\tGTF already imported')\n",
    "        return\n",
    "    gene_count = 0\n",
    "    tx_count = 0\n",
    "    ex_count = 0\n",
    "    with _con:\n",
    "        _con.execute('INSERT INTO datasets VALUES (?, ?)', (name, date.today()))\n",
    "        for line in f:\n",
    "            if line[0] == '#': continue\n",
    "            feat = parse_gtf_line(line)\n",
    "            try:\n",
    "                if feat['feature_type'] == 'gene':\n",
    "                    _con.execute('INSERT INTO genes VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', (\n",
    "                        *list(feat.values())[:8],\n",
    "                        json.dumps(feat['attributes']),\n",
    "                        name,\n",
    "                        feat['attributes']['gene_id'],\n",
    "                        feat['attributes'].get('gene_name')\n",
    "                    ))\n",
    "                    gene_count += 1\n",
    "                elif feat['feature_type'] == 'transcript':\n",
    "                    _con.execute('INSERT INTO transcripts VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', (\n",
    "                        *list(feat.values())[:8],\n",
    "                        json.dumps(feat['attributes']),\n",
    "                        name,\n",
    "                        feat['attributes']['gene_id'],\n",
    "                        feat['attributes'].get('gene_name'),\n",
    "                        feat['attributes']['transcript_id']\n",
    "                    ))\n",
    "                    tx_count += 1\n",
    "                    if tx_count % 1000 == 0:\n",
    "                        print('\\tread', tx_count, 'transcripts from file')\n",
    "                elif feat['feature_type'] == 'exon':\n",
    "                    _con.execute('INSERT INTO exons VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)', (\n",
    "                        *list(feat.values())[:8],\n",
    "                        json.dumps(feat['attributes']),\n",
    "                        name,\n",
    "                        feat['attributes']['gene_id'],\n",
    "                        feat['attributes'].get('gene_name'),\n",
    "                        feat['attributes']['transcript_id'],\n",
    "                        feat['attributes'].get('exon_id'),\n",
    "                        feat['attributes']['exon_number']\n",
    "                    ))\n",
    "                    ex_count += 1\n",
    "            except sqlite3.IntegrityError:\n",
    "                pprint(feat)\n",
    "                raise\n",
    "    print(f'\\tinserted {gene_count} genes/{tx_count} transcripts/{ex_count} exons into db')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tread 1000 transcripts from file\n",
      "\tread 2000 transcripts from file\n",
      "\tread 3000 transcripts from file\n",
      "\tread 4000 transcripts from file\n",
      "\tread 5000 transcripts from file\n",
      "\tread 6000 transcripts from file\n",
      "\tread 7000 transcripts from file\n",
      "\tread 8000 transcripts from file\n",
      "\tread 9000 transcripts from file\n",
      "\tread 10000 transcripts from file\n",
      "\tread 11000 transcripts from file\n",
      "\tread 12000 transcripts from file\n",
      "\tread 13000 transcripts from file\n",
      "\tread 14000 transcripts from file\n",
      "\tread 15000 transcripts from file\n",
      "\tread 16000 transcripts from file\n",
      "\tread 17000 transcripts from file\n",
      "\tread 18000 transcripts from file\n",
      "\tread 19000 transcripts from file\n",
      "\tread 20000 transcripts from file\n",
      "\tread 21000 transcripts from file\n",
      "\tread 22000 transcripts from file\n",
      "\tread 23000 transcripts from file\n",
      "\tread 24000 transcripts from file\n",
      "\tread 25000 transcripts from file\n",
      "\tread 26000 transcripts from file\n",
      "\tread 27000 transcripts from file\n",
      "\tread 28000 transcripts from file\n",
      "\tread 29000 transcripts from file\n",
      "\tread 30000 transcripts from file\n",
      "\tread 31000 transcripts from file\n",
      "\tread 32000 transcripts from file\n",
      "\tread 33000 transcripts from file\n",
      "\tread 34000 transcripts from file\n",
      "\tread 35000 transcripts from file\n",
      "\tread 36000 transcripts from file\n",
      "\tread 37000 transcripts from file\n",
      "\tread 38000 transcripts from file\n",
      "\tread 39000 transcripts from file\n",
      "\tread 40000 transcripts from file\n",
      "\tread 41000 transcripts from file\n",
      "\tread 42000 transcripts from file\n",
      "\tread 43000 transcripts from file\n",
      "\tread 44000 transcripts from file\n",
      "\tread 45000 transcripts from file\n",
      "\tread 46000 transcripts from file\n",
      "\tread 47000 transcripts from file\n",
      "\tread 48000 transcripts from file\n",
      "\tread 49000 transcripts from file\n",
      "\tread 50000 transcripts from file\n",
      "\tread 51000 transcripts from file\n",
      "\tread 52000 transcripts from file\n",
      "\tread 53000 transcripts from file\n",
      "\tread 54000 transcripts from file\n",
      "\tread 55000 transcripts from file\n",
      "\tread 56000 transcripts from file\n",
      "\tread 57000 transcripts from file\n",
      "\tread 58000 transcripts from file\n",
      "\tread 59000 transcripts from file\n",
      "\tread 60000 transcripts from file\n",
      "\tread 61000 transcripts from file\n",
      "\tread 62000 transcripts from file\n",
      "\tread 63000 transcripts from file\n",
      "\tread 64000 transcripts from file\n",
      "\tread 65000 transcripts from file\n",
      "\tread 66000 transcripts from file\n",
      "\tread 67000 transcripts from file\n",
      "\tread 68000 transcripts from file\n",
      "\tread 69000 transcripts from file\n",
      "\tread 70000 transcripts from file\n",
      "\tread 71000 transcripts from file\n",
      "\tread 72000 transcripts from file\n",
      "\tread 73000 transcripts from file\n",
      "\tread 74000 transcripts from file\n",
      "\tread 75000 transcripts from file\n",
      "\tread 76000 transcripts from file\n",
      "\tread 77000 transcripts from file\n",
      "\tread 78000 transcripts from file\n",
      "\tread 79000 transcripts from file\n",
      "\tread 80000 transcripts from file\n",
      "\tread 81000 transcripts from file\n",
      "\tread 82000 transcripts from file\n",
      "\tread 83000 transcripts from file\n",
      "\tread 84000 transcripts from file\n",
      "\tread 85000 transcripts from file\n",
      "\tread 86000 transcripts from file\n",
      "\tread 87000 transcripts from file\n",
      "\tread 88000 transcripts from file\n",
      "\tread 89000 transcripts from file\n",
      "\tread 90000 transcripts from file\n",
      "\tread 91000 transcripts from file\n",
      "\tread 92000 transcripts from file\n",
      "\tread 93000 transcripts from file\n",
      "\tread 94000 transcripts from file\n",
      "\tread 95000 transcripts from file\n",
      "\tread 96000 transcripts from file\n",
      "\tread 97000 transcripts from file\n",
      "\tread 98000 transcripts from file\n",
      "\tread 99000 transcripts from file\n",
      "\tread 100000 transcripts from file\n",
      "\tread 101000 transcripts from file\n",
      "\tread 102000 transcripts from file\n",
      "\tread 103000 transcripts from file\n",
      "\tread 104000 transcripts from file\n",
      "\tread 105000 transcripts from file\n",
      "\tread 106000 transcripts from file\n",
      "\tread 107000 transcripts from file\n",
      "\tread 108000 transcripts from file\n",
      "\tread 109000 transcripts from file\n",
      "\tread 110000 transcripts from file\n",
      "\tread 111000 transcripts from file\n",
      "\tinserted 14699 genes/111442 transcripts/787397 exons into db\n"
     ]
    }
   ],
   "source": [
    "gtf = Path('/mnt/z/Gandal/GTF/All_filtered_118k_multiexon_A0.75_minRead3_2datasetsupport_talon.gtf')\n",
    "with open(gtf) as f:\n",
    "    import_gtf(f, gtf.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf = Path('/mnt/z/Gandal/GTF/All_filtered_118k_multiexon_A0.75_minRead3_2datasetsupport_talon.gtf')\n",
    "with open(gtf) as f:\n",
    "    import_gtf(f, gtf.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dataset(name):\n",
    "    with _con:\n",
    "        _con.execute('DELETE FROM exons WHERE dataset=?', (name,))\n",
    "        _con.execute('DELETE FROM transcripts WHERE dataset=?', (name,))\n",
    "        _con.execute('DELETE FROM genes WHERE dataset=?', (name,))\n",
    "        _con.execute('DELETE FROM datasets WHERE name=?', (name,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_model_exons():\n",
    "    with _con:\n",
    "        _con.execute('DELETE FROM model_exons')\n",
    "drop_model_exons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_dataset(gtf.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gtf in data_dir.glob('*.gtf'):\n",
    "    print('Processing', gtf.name)\n",
    "    with open(gtf) as f:\n",
    "        mongo_insert_gtf(f, gtf.stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_union(intervals):\n",
    "    \"\"\"\n",
    "    Returns the union of all intervals in the input list\n",
    "      intervals: list of tuples or 2-element lists\n",
    "    \"\"\"\n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "    union = [intervals[0]]\n",
    "    for i in intervals[1:]:\n",
    "        if i[0] <= union[-1][1]:  # overlap w/ previous\n",
    "            if i[1] > union[-1][1]:  # only extend if larger\n",
    "                union[-1][1] = i[1]\n",
    "        else:\n",
    "            union.append(i)\n",
    "    return union\n",
    "\n",
    "def mongo_bulk_write_model(ops, count=0):\n",
    "    try:\n",
    "        _db.model_exons.bulk_write(ops, ordered=False)\n",
    "    except BulkWriteError as bwe:\n",
    "        pprint(bwe.details)\n",
    "    print('\\twrote', count+len(ops), 'genes to db')\n",
    "    return len(ops)\n",
    "\n",
    "def generate_model_exons(dataset):\n",
    "    #genes = _db.genes.distinct('gene.gene_name', {'file': file1})\n",
    "    genes = [x[0] for x in _con.execute('SELECT DISTINCT gene_name FROM genes WHERE dataset=?', (dataset,))]\n",
    "    #filter = {'file': {'$in': files2}} if files2 else {}\n",
    "    print('Found', len(genes), 'distinct genes in', dataset)\n",
    "    gene_count = 0\n",
    "    ex_count = 0\n",
    "    with _con:\n",
    "        for gene in genes:\n",
    "            chrom = None\n",
    "            strand = None\n",
    "            exon_coords = []\n",
    "            rows = _con.execute(\"\"\"SELECT\n",
    "                chromosome,\n",
    "                start,\n",
    "                end,\n",
    "                strand,\n",
    "                json_extract(attributes, '$.transcript_type')\n",
    "            FROM exons WHERE gene_name=?\"\"\", (gene,))\n",
    "            for row in rows:\n",
    "                if not chrom:\n",
    "                    chrom = row[0]\n",
    "                if not strand:\n",
    "                    chrom = row[3]\n",
    "                if row[4] == 'retained_intron':\n",
    "                    continue\n",
    "                exon_coords.append([row[1], row[2]])\n",
    "                ex_count += 1\n",
    "            new_coords = interval_union(exon_coords)\n",
    "            #start_pos = np.min([i[0] for i in new_coords])\n",
    "            #end_pos = np.max([i[1] for i in new_coords])\n",
    "            if strand == '-':\n",
    "                new_coords.reverse()\n",
    "            for i, (start, end) in enumerate(new_coords, 1):\n",
    "                _con.execute('INSERT INTO model_exons VALUES (?, ?, ?, ?, ?, ?, ?)', (\n",
    "                    chrom,\n",
    "                    start,\n",
    "                    end,\n",
    "                    strand,\n",
    "                    gene,\n",
    "                    '_'.join([gene, str(i)]),\n",
    "                    i\n",
    "                ))\n",
    "            gene_count += 1\n",
    "            if gene_count % 1000 == 0:\n",
    "                print('Processed', gene_count, 'genes')\n",
    "    print('Generated', ex_count, 'exons across', gene_count, 'genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 14690 distinct genes in All_filtered_118k_multiexon_A0.75_minRead3_2datasetsupport_talon\n",
      "Processed 1000 genes\n",
      "Processed 2000 genes\n",
      "Processed 3000 genes\n",
      "Processed 4000 genes\n",
      "Processed 5000 genes\n",
      "Processed 6000 genes\n",
      "Processed 7000 genes\n",
      "Processed 8000 genes\n",
      "Processed 9000 genes\n",
      "Processed 10000 genes\n",
      "Processed 11000 genes\n",
      "Processed 12000 genes\n",
      "Processed 13000 genes\n",
      "Processed 14000 genes\n",
      "Generated 1635560 exons across 14690 genes\n"
     ]
    }
   ],
   "source": [
    "generate_model_exons('All_filtered_118k_multiexon_A0.75_minRead3_2datasetsupport_talon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_expression_values(f, dataset):\n",
    "    tx = {}\n",
    "    for i,line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        tx_id = tokens[3].replace('-', '_')\n",
    "        cell_type = tokens[4]\n",
    "        avg_exp = float(tokens[1])\n",
    "        pct_exp = float(tokens[2])\n",
    "        avg_exp_scaled = float(tokens[5])\n",
    "        if tx_id not in tx:\n",
    "            tx[tx_id] = {}\n",
    "        tx[tx_id][cell_type] = [avg_exp, pct_exp, avg_exp_scaled]\n",
    "    print(len(tx), 'total transcripts')\n",
    "    write_count = 0\n",
    "    with _con:\n",
    "        for tx_id in tx:\n",
    "            exp = [dict(zip(['cell_type', 'avg_exp', 'pct_exp', 'avg_exp_scaled'], [k]+v)) for k,v in tx[tx_id].items()]\n",
    "            cur = _con.execute(\"UPDATE transcripts SET attributes=(SELECT json_set(attributes, '$.expression', json('\"+json.dumps(exp)+\"')) FROM transcripts) WHERE transcript_id=? AND dataset=?\", (tx_id, dataset))\n",
    "            write_count += cur.rowcount\n",
    "            if cur.rowcount > 0 and write_count % 1000 == 0:\n",
    "                print('wrote', write_count, 'transcripts to db')\n",
    "    if write_count % 1000 != 0:\n",
    "        print('wrote', write_count, 'transcripts to db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "112134 total transcripts\n",
      "wrote 1000 transcripts to db\n",
      "wrote 2000 transcripts to db\n",
      "wrote 3000 transcripts to db\n",
      "wrote 4000 transcripts to db\n",
      "wrote 5000 transcripts to db\n",
      "wrote 6000 transcripts to db\n",
      "wrote 7000 transcripts to db\n",
      "wrote 8000 transcripts to db\n",
      "wrote 9000 transcripts to db\n",
      "wrote 10000 transcripts to db\n",
      "wrote 11000 transcripts to db\n",
      "wrote 12000 transcripts to db\n",
      "wrote 13000 transcripts to db\n",
      "wrote 13000 transcripts to db\n",
      "wrote 14000 transcripts to db\n",
      "wrote 15000 transcripts to db\n",
      "wrote 16000 transcripts to db\n",
      "wrote 17000 transcripts to db\n",
      "wrote 18000 transcripts to db\n",
      "wrote 19000 transcripts to db\n",
      "wrote 20000 transcripts to db\n",
      "wrote 21000 transcripts to db\n",
      "wrote 22000 transcripts to db\n",
      "wrote 23000 transcripts to db\n",
      "wrote 24000 transcripts to db\n",
      "wrote 25000 transcripts to db\n",
      "wrote 26000 transcripts to db\n",
      "wrote 27000 transcripts to db\n",
      "wrote 28000 transcripts to db\n",
      "wrote 29000 transcripts to db\n",
      "wrote 29000 transcripts to db\n",
      "wrote 30000 transcripts to db\n",
      "wrote 31000 transcripts to db\n",
      "wrote 32000 transcripts to db\n",
      "wrote 33000 transcripts to db\n",
      "wrote 34000 transcripts to db\n",
      "wrote 35000 transcripts to db\n",
      "wrote 36000 transcripts to db\n",
      "wrote 37000 transcripts to db\n",
      "wrote 38000 transcripts to db\n",
      "wrote 39000 transcripts to db\n",
      "wrote 40000 transcripts to db\n",
      "wrote 41000 transcripts to db\n",
      "wrote 42000 transcripts to db\n",
      "wrote 43000 transcripts to db\n",
      "wrote 44000 transcripts to db\n",
      "wrote 45000 transcripts to db\n",
      "wrote 46000 transcripts to db\n",
      "wrote 47000 transcripts to db\n",
      "wrote 48000 transcripts to db\n",
      "wrote 49000 transcripts to db\n",
      "wrote 50000 transcripts to db\n",
      "wrote 51000 transcripts to db\n",
      "wrote 52000 transcripts to db\n",
      "wrote 53000 transcripts to db\n",
      "wrote 54000 transcripts to db\n",
      "wrote 55000 transcripts to db\n",
      "wrote 56000 transcripts to db\n",
      "wrote 57000 transcripts to db\n",
      "wrote 58000 transcripts to db\n",
      "wrote 59000 transcripts to db\n",
      "wrote 60000 transcripts to db\n",
      "wrote 61000 transcripts to db\n",
      "wrote 62000 transcripts to db\n",
      "wrote 63000 transcripts to db\n",
      "wrote 64000 transcripts to db\n",
      "wrote 65000 transcripts to db\n",
      "wrote 66000 transcripts to db\n",
      "wrote 67000 transcripts to db\n",
      "wrote 68000 transcripts to db\n",
      "wrote 69000 transcripts to db\n",
      "wrote 70000 transcripts to db\n",
      "wrote 71000 transcripts to db\n",
      "wrote 72000 transcripts to db\n",
      "wrote 73000 transcripts to db\n",
      "wrote 74000 transcripts to db\n",
      "wrote 75000 transcripts to db\n",
      "wrote 76000 transcripts to db\n",
      "wrote 77000 transcripts to db\n",
      "wrote 78000 transcripts to db\n",
      "wrote 79000 transcripts to db\n",
      "wrote 80000 transcripts to db\n",
      "wrote 81000 transcripts to db\n",
      "wrote 82000 transcripts to db\n",
      "wrote 83000 transcripts to db\n",
      "wrote 84000 transcripts to db\n",
      "wrote 85000 transcripts to db\n",
      "wrote 86000 transcripts to db\n",
      "wrote 87000 transcripts to db\n",
      "wrote 88000 transcripts to db\n",
      "wrote 89000 transcripts to db\n",
      "wrote 90000 transcripts to db\n",
      "wrote 91000 transcripts to db\n",
      "wrote 92000 transcripts to db\n",
      "wrote 93000 transcripts to db\n",
      "wrote 94000 transcripts to db\n",
      "wrote 95000 transcripts to db\n",
      "wrote 96000 transcripts to db\n",
      "wrote 97000 transcripts to db\n",
      "wrote 98000 transcripts to db\n",
      "wrote 99000 transcripts to db\n",
      "wrote 100000 transcripts to db\n",
      "wrote 101000 transcripts to db\n",
      "wrote 102000 transcripts to db\n",
      "wrote 103000 transcripts to db\n",
      "wrote 104000 transcripts to db\n",
      "wrote 105000 transcripts to db\n",
      "wrote 106000 transcripts to db\n",
      "wrote 107000 transcripts to db\n",
      "wrote 108000 transcripts to db\n",
      "wrote 108128 transcripts to db\n"
     ]
    }
   ],
   "source": [
    "with open('/mnt/z/Gandal/expression/Isoform_Average_percent_expression.txt') as f:\n",
    "    import_expression_values(f, 'All_filtered_118k_multiexon_A0.75_minRead3_2datasetsupport_talon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_selectize(f, dataset):\n",
    "    selectize = sorted([x[0] for x in _con.execute('SELECT DISTINCT gene_name FROM genes WHERE dataset=?', (dataset,))])\n",
    "    f.write('function getSelectizeOptions(){ return ')\n",
    "    json.dump([{'v': x} for x in selectize], f)\n",
    "    f.write('; }')\n",
    "    if hasattr(f, 'name'):\n",
    "        print('Wrote', len(selectize), 'gene symbols to', f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also close the connection if we are done with it.\n",
    "# Just be sure any changes have been committed or they will be lost.\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}